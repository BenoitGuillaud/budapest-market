---
title: 'Budapest Property Market: Tools for Predictive Modelling'
author: "Benoît Guillaud"
date: "21 June 2017"
output:
  html_document:
    toc: yes
---

```{r setup, echo=FALSE, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

# start with a clean slate
gc()            # Force R to release memory it is no longer using
rm(list = ls()) # Delete all objects in the workspace 

# load libraries
source("./R/LoadLibraries.R")

# load custom functions
source("./R/EvaluateModel.R")
source("./R/PrepareData.R")
source("./R/PlotMissingPercentages.R")
```

# Abstract
This is the last in a series of three papers aimed to illustrate the use of simple R tools for the analysis of a city's Real Property market. We previously covered Exploratory Data Analysis (Paper 1) and built predictive models using machine learning algorithms (Paper 2). In this third paper, we use the models of projected rental income and property prices as surrogates in an optimization routine in order to find the best return on investment in the design space.

The optimization provides clear insights to the investor to obtain the best returns: the best locations in the city, whether a lift is important, and whether to opt for a large or small flat. 

Along the way, we will demonstrate how to use `mlrMBO::` package, a model-based optimization framework in R.

# Problem definition
In this optimization problem, we are looking to maximise the return on investment defined as the annual income from renting a flat divided by the initial property price. The variables of interest are:

* Flat surface area (continuous, between 50 m2 and 120 m2)
* Whether there is a lift or not (categorical, Yes/No)
* The district where te flat is located (categorical, 4 different város)

Note that each variable has a distinct type: continuous or categorical. They can take any value within the range/list specified.

There are no optimization constraint per se. A constraint would be useful if you had a defined budget not to exceed or a minimum investment target.

# Surrogate models
Let's create simple models for (1) the annual rental income and (2) the property price. We will use them as surrogate "black-box" models in the optimization routine.

The code below loads and prepares the datasets, then trains the models with a simple linear regression. 
```{r, echo=FALSE}
# Load the data
elado <- read.csv("./data/extraction_elado_2017_03_16.txt", 
                  fileEncoding="UTF-8-BOM", header=TRUE, sep=";")
kiado <- read.csv("./data/extraction_kiado_2017_03_16.txt", 
                  fileEncoding="UTF-8-BOM", header=TRUE, sep=";")

# Clean and recode the variables
elado <- PrepareData(elado)
kiado <- PrepareData(kiado)

# Create new variable ppsm and rpsm from "price"
elado <- dplyr::mutate(elado, ppsm = price*1000/area)    # kFt/m2
kiado <- dplyr::mutate(kiado, 
                       rent = price*12/1000,             # mFt/year
                       rpsm = price*12/area)             # kFt/m2/year

# select the features of interest
elado <- dplyr::select(elado, price, area, varos, lift)
kiado <- dplyr::select(kiado, rent, area, varos, lift)

# drop levels except "Belso-Erzsébetváros", "Belso-Terézváros", "Belváros", "Lipótváros"
nonWantedLevels<-c(4, 5, 6, 7, 8, 9, 11)
elado <- elado %>%
  dplyr::filter(!as.integer(varos) %in% nonWantedLevels) %>%
  droplevels()
nonWantedLevels<-c(4, 5, 6, 7, 8)
kiado <- kiado %>%
  dplyr::filter(!as.integer(varos) %in% nonWantedLevels) %>%
  droplevels()

```

```{r, echo=FALSE}
# elado
set.seed(11)
inTrain <- caret::createDataPartition(elado$price, p=0.75,list = FALSE)
elado.train <- elado[inTrain,]
elado.test <-  elado[-inTrain,]

# kiado
set.seed(11)
inTrain <- caret::createDataPartition(kiado$rent, p=0.75,list = FALSE)
kiado.train <- kiado[inTrain,]
kiado.test <-  kiado[-inTrain,]
```

Linear regression models, using 3 features (varos, area, lift):
```{r, results="hide"}
# train linear regression model
price_mod_lm.A <- stats::lm(price ~ area + varos + lift,
                            data = elado.train,
                            na.action = na.omit)

# model summary                       
summary(price_mod_lm.A) 

# Predictions on testing set
price_pred_lm.A <- predict(price_mod_lm.A, newdata=na.omit(elado.test))

# # evaluate model on testing set
# with(subset(na.omit(elado.test)),
#      EvaluateModel(price, price_pred_lm.A))
```

```{r, results="hide", message=FALSE}
# train linear regression model
rent_mod_lm.A <- stats::lm(rent ~ area + varos + lift,
                            data = kiado.train,
                            na.action = na.omit)
# model summary                       
summary(rent_mod_lm.A) 

# Predictions on testing set
rent_pred_lm.A <- predict(rent_mod_lm.A, newdata=na.omit(kiado.test))

# # evaluate model on testing set
# with(subset(na.omit(kiado.test)),
#      EvaluateModel(rent, rent_pred_lm.A))
```

Note that these very simple models (linear regression from 3 features) very poorly fit the data.

# Optimization with mlrMBO
The following steps are needed to start the optimization:

* Define the objective function and its parameters using the package smoof.
* Generate an initial design (optional).
* Define a mlr learner for the surrogate model (optional).
* Set up a MBO control object.
* Finally start the optimization with mbo().


```{r}
# define objective function
fun = function(x) {
  df <- data.frame(area = x$area,
                     lift = x$lift,
                     varos = x$varos)
  pp <- predict(price_mod_lm.A, newdata=df)
  rr <- predict(rent_mod_lm.A, newdata=df)

  anr = rr/pp * 100
  return(anr)
}

obj.fun = makeSingleObjectiveFunction(
  name = "Annual return",
  fn = fun,
  par.set = makeParamSet(
    makeNumericParam("area", lower = 50, upper = 100),
    makeDiscreteParam("lift", values = c("nincs", "van")),
    makeDiscreteParam("varos", values = c("Belso-Erzsébetváros", "Belso-Terézváros",
                                          "Belváros", "Lipótváros"))
  ),
  has.simple.signature = FALSE,
  minimize = FALSE
)

# visualize the function
autoplot(obj.fun)

```


The objective function plots show at a glance that the best returns are achieved in Belso-Erzsébetváros, for the smaller flat without any lift. The visualization is easy with 3 features. The `mbo()` routine below becomes mandatory when the prediction is based on more variables.


```{r, echo=TRUE, results="hide", message=FALSE}
# generate initial design (optional)
des = generateDesign(n = 20, par.set = getParamSet(obj.fun), fun = lhs::randomLHS)

# define mlr learner for surrogate model (optional)
mlr::configureMlr(show.info = FALSE, show.learner.output = FALSE, on.learner.warning = "quiet")
surr.rf = makeLearner("regr.randomForest", predict.type = "se")

# set MBO control object
mbo.ctrl = makeMBOControl()
mbo.ctrl = setMBOControlTermination(mbo.ctrl, iters = 100)
mbo.ctrl = setMBOControlInfill(mbo.ctrl, crit = makeMBOInfillCritEI())

# start optimization with mbo().
run = mbo(obj.fun, control = mbo.ctrl, design = des, learner = surr.rf,  show.info = TRUE)

```

```{r}
# diagnostic
print(run)
#plot(run)
```
As expected, the output of the optimization recommends to invest into a small flat, without a lift, in elso-Erzsébetváros.


# Conclusions
The analysis above illustrates the power of the `mlrMBO` framework. Other optimization packages are available in R but I found `mlrMBO` particularly apt at handling the problem at hand:

* Global optimization problem, with potentially multiple maxima and minima
* Non-smooth problem, because of the categorical variables: the objective function does not possess derivatives
* Optimization under (linear or non-linear) constraints
* Handling of "black-box" models - the surrogates


Additional references for optimization in R include:

* mlrMBO, a toolbox for model-based optimization https://cran.r-project.org/web/packages/mlrMBO/index.html
* ROI, a framework for handling optimization problems https://cran.r-project.org/web/packages/ROI/index.html and https://www.r-project.org/conferences/useR-2010/slides/Theussl+Hornik+Meyer.pdf
* Book on optimization with R: https://www.amazon.com/Nonlinear-Parameter-Optimization-Using-Tools/dp/1118569288 (`optimx` package) and http://www3.dsi.uminho.pt/pcortez/mor/ (`tabuSearch` package)
* Other good reads: https://fr.mathworks.com/help/gads/index.html and https://cran.r-project.org/web/views/Optimization.html

